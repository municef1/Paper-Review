# [1] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep learning." Nature 521.7553 (2015): 436-444. [pdf](cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) (Three Giants' Survey)

- Three Giants' Survey는 Yann LeCun, Yoshua Bengio, Geoffrey Hinton이 저술하고 Nature지에 게재된 리뷰 논문이다.
- 딥 러닝을 소개하고 고전적인 기계학습과 구별한다.
- 역전파와 컨볼루션 네트워크와 같은 중요 기술들과 아키텍쳐에 대해 논의한다.
- [원문 링크](cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)

# 1. Abstract
1. 딥러닝을 사용한 모델들은 여러 레벨의 추출을 통해 데이터를 학습할 수 있다.

2. 이 이론은 음성 인식, 객체 인식, 객체 감지, 약물 발견, 유전체학과 같은 여러 영역의 기술들을 획기적으로 발전시켰다.

3. 딥러닝은 이전 레이어에서 각 레이어를 계산하는데 사용되는 내부 매개변수(파라미터)를 어떻게 변경해야 하는지를 나타내는 역전파 알고리즘을 사용하여 대규모의 데이터셋의 복잡한 구조를 발견한다.

4. CNN은 이미지, 비디오, 오디오 처리 / RNN은 텍스트나 음성 처리에 획기적인 발전을 가져왔다.


# 2. Intro
- 기계학습은 검색부터 sns필터링, 전자 상거래 웹 사이트에서의 추천시스템까지 여러 측면에 영향을 끼치고 있고, 딥러닝이라는 기술을 점점 더 많이 사용하게 되고 있다.  
- 딥러닝을 쓰는 가장 큰 이유 중 하나는 딥러닝 모델이 원 데이터(raw data)를 입력했을 때 적절한 데이터 표현을 자동으로 발견한다는 점이다.  
- 이로 인해 이전의 기계학습에서 필요하던 전문가의 도메인 지식이 크게 줄어들었다. 

- 딥러닝 모델은 단순하지만 비선형이 모듈을 쌓아서 생성된다.
- 각 모듈은 이전 단계에서 더 낮은 수준, 약간 덜 추상적인 표현으로 변환한다.
- 딥러닝의 핵심은 각 계층의 기능을 인간이 설계하지 않는다는 점이다.
- 각 데이터셋에 대해 수동으로 조정할 필요 없이 원 데이터에 적용할 수 있다.
- 때문에 계산능력을 활용하여 학습 효율성과 성능을 높일 수 있다.
 

# 3. Supervised Learning (지도 학습)
- 지도 학습
  - 데이터 셋의 입력에서 "ground-truth 레이블"이 필요하다.
  - 정확하게 예측할 수 있도록 돕는 "올바른 레이블", 즉, 숫자, 클래스, 데이터 튜플 등의 "정답지"를 입력해 학습시키는 것을 의미한다.
- 학습을 하는 동안에는 입력에 따른 출력(결과물)이 "올바른 레이블"과 비교된다.
- 그 다음 목적 함수(오차 또는 손실 함수)를 사용해 모델 출력과 "올바른 레이블" 사이의 오차(거리)를 측정한다.
- 이 오류를 받아서 가중치라고 부르는 모델 내부의 조정 가능한 매개변수(파라미터)를 조정해 오류를 줄인다.
  - 각 가중치에 대해 가중치가 약간 증가할 경우 오류가 증거하거나 감소하는 양을 나타내는 gradient vector를 계산한다.

- 일반적인 딥러닝 모델에는 모델의 입-출력 매핑을 정의하는 수억개의 가중치가 존재한다.
- 모델 훈련에 가장 일반적으로 사용되는 방법은 경사 하강법(Gradient-Descending, GD)이다.
  - GD의 목표는 1차 도함수(기울기)를 사용해 오류(손실) 함수를 최소화시키는 것이다.
  - 업데이트 할 파라미터에 대해 손실 함수의 기울기를 계산한 다음 파라미터를 조정해 기울기의 반대방향으로 조금 이동한다.
- 가장 유명한 GD 유형은 BGD(Batch Gradient Descending), SGD, MBGD(Mini Batch Gradient Descending)이다.
  - BGD는 전체 데이터셋의 오차를 평균화한 다음 이를 기반으로 기울기를 계산한다.
  - SGD는 기울기가 각 포인트에 대해 계산된 다음, 가중치를 업데이트에 사용한다. (따라서 데이터셋 크기 N에 대한 파라미터는 각 반복당 N번 업데이트된다.)
  - MBGD는 두 가지 방법을 혼합한 것으로, 전체 데이터셋에 대한 오차를 필요로하지 않으면서도, "벡터화"를 사용해 기울기를 계산하는 속도를 높일 수 있기 때문에 BGD보다 빠르다.
    - "벡터화" : GPU나 CPU를 사용해 여러 데이터 조각에 대해 동시에 연산을 수행할 수 있는 프로그래밍 스타일을 의미한다.
    - 또, 많은 논문에서 MBGD를 SGD로 언급한다. 둘 다 빈번하면서도 가중치 업데이트라는 같은 개념을 중심으로 진행되기 때문이다.
- 학습 후 모델의 성능은 테스트 세트라는 학습 데이터셋과는 다른 데이터셋에서 측정된다.
  - 모델의 일반화 능력, 즉, 학습하면서 본 적 없던 입력에 대해서도 올바른 출력을 뱉어내는지를 테스트하고자 한다.
- 이전의 분류 모델들은 식별에 필요한 상당한 양의 엔지니어링 기술과 전문 도메인 지식을 토대로 직접 설계해야 했다.

  <img src = ./imgs/1.png width="600" height="300">
  
- 픽셀 수준에서 봤을 때 다른 포즈, 다른 배경에 있는 이미지들은 서로 매우 다를 수 있다.
  - 때문에 기존의 분류 모델들은 분류를 위해 아주 많은 테크닉이 적용되어야 했다.
- 딥러닝 모델은 앞에서 설명한 학습 과정을 통해 자동으로 기능을 학습할 수 있기에, 이러한 점들을 피할 수 있다.
- 딥러닝 모델은 단순한 모듈을 여러 층으로 쌓은 기계학습 모델으로, 전체 또는 대부분이 학습 대상이며, 비선형 입출력 매핑을 계산해 진행된다.


# 4. Backpropagation to train multilayer architectures (다충 구조 학습을 위한 역전파)
- 가장 가장 기본적인 딥러닝 모델은 고정된 사이즈의 입력을 고정된 사이즈의 출력으로 매핑하는 방법을 학습하는, 피드포워드(feed forward) 신경망 아키텍처이다.

- 한 레이어에서 다음 레이어로 이동하기 위해 이전 입력에 대한 가중 합계를 계산하고, 그 결과를 비선형 함수를 통해 전달한다.
  - 가장 흔히 사용되는 비선형 함수는 ReLU(Rectified Linear Unit)으로, 출력이 음수이면 0을 출력하고, 그렇지 않으면 입력을 그대로 전달한다.
  - 이전까지는 tanh 또는 sigmoid와 같은 부드러운 비선형 함수를 사용했지만, 딥러닝에서 훨씬 빠르게 학습할 수 있도록 만들어주는 ReLU를 사용하게 됐다.

- 조정하고자 하는 파라미터에 대해 오차 함수의 기울기가 계산되는 역전파 과정은 딥러닝의 핵심 중 하나이다.
- 역전파의 핵심은 계층의 입력에 대한 오차의 기울기를 해당 계층의 출력에 대한 기울기에서 거꾸로 계산할 수 있다는 것이다.
  <img src = ./imgs/2.png width="600" height="600">

- 입력은 입력 레이어로 들어가고, 히든 레이어로 전송되어 해당 레이어의 가중치와 곱해지고, 해당 레이어의 편향으로 합산된다.
- 그 다음 출력은 일부 비선형함수(활성화 함수)로 전송된 다음 다음 계층으로 넘어간다.
- 마지막으로 출력 레이어는 네트워크의 예측값을 출력해낸다.
- 역전파 방정식은 네트워크가 예측을 생성해내는 맨 위의 출력에서 시작해서, 입력이 들어오는 맨 아래까지 기울기를 모든 레이어에 전파하기 위해 반복적으로 적용할 수 있다.
  - 이러한 기울기가 계산되면 각 모듈의 가중치에 대한 기울기를 계산하는 것은 간단해진다.

- 1990년대 후반에는 신경망과 역전파에 대해서는 대부분 부정적이었다.
- 단순한 경사하강법은 오류의 실제 미니멈(정답지가 있는 곳, global minimum)이 아닌 로컬 미니멈에 갇힐 것이라고 일반적으로 생각했다.
- 하지만 분석해봤을 때 안장점(saddle point)이 매우 많이 존재하는 것처럼 보이지만 거의 모두 목적함수의 값이 매우 비슷했다.
  - 따라서 안장점 중 어느 지점에서 알고리즘이 멈추는지는 그다지 중요치 않게 됐다.
  - 안장점 : 함수의 그래프 표면에서 직교 방향의 기울기가 모두 0이지만 극대, 극소점은 아닌 부분.
  - (안장점 그림 넣기)

- 2006년 CIFAR(Canadian Institute for Advanced Research)에서 심층 피드포워드 네트워크에 대한 관심이 되살아났다.
- 레이블이 지정된 데이터 없이도 특징 추출 레이어를 생성할 수 있는 비지도 학습 과정을 도입했다.
- 특징 추출기의 각 레이어를 학습하는 목적은 아래 레이어에서 특징 추출기를 모델링할 수 있도록 하는 것이었다.
- 특징 추출기의 여러 레이어를 '사전 훈련'함으로써 심층 네트워크의 가중치를 합리적인 값으로 초기화할 수 있었다.
- 그런 다음 출력 단위의 최종 계층을 네트워크 상단에 추가하고 전파(Propagation)을 사용하여 전체 시스템을 미세 조정할 수 있다.

- 하지만 인접한 레이어 사이에 완전한 연결이 있는 네트워크보다 훨씬 잘 훈련되고 일반화되는 유형의 피드포워드 네트워크가 생겼다.
- 이것이 바로 컨볼루션 신경망(ConvNet)이다.


# 5. Convolutional neural networks (컨볼루션 신경망)

-  ConvNet 다차원 데이터 구조의 형태(텐서)로 제공되는 데이터를 처리하도록 설계되었다.
   - RGB 이미지는 높이, 넓이, 깊이(채널)의 3D텐서로 표현된다.
   - 신호, 시퀀스(언어), 이미지, 오디오, 비디오처럼 여러 데이터 형식이 존재하며, 다차원 데이터 구조로 표현된다.
   - ConvNet에는 로컬 연결, 가중치 공유, 풀링, 여러 레이어의 사용과 같은 데이터의 속성을 활용하는 주요 아이디어가 있다.

- 일반적인 ConvNet의 아키텍처는 컨볼루션 레이어와 풀링 레이어로 구성된다.

<img src = ./imgs/3.png width="800" height="400">

- 컨볼루션 레이어의 유닛은 피처 맵으로 구성되며 각 유닛은 필터 뱅크라고 불리는 일련의 가중치를 통해 이전 레이어의 피처 맵에 있는 로컬 패치에 연결된다.
- 이 로컬 가중치의 합계 결과는 ReLU와 같은 비선형 함수를 통해 전달된다.
- 피처 맵의 모든 단위는 동일한 필터 뱅크를사용하며, 레이어의 서로 다른 피처 맵은 다른 필터 뱅크를 사용한다.
  
- 이러한 아키텍처가 만들어지는 이유는 두 가지이다.
  - 1. 이미지와 같은 배열 데이터에서 값의 로컬 그룹은 종종 높은 상관 관계가 있어 쉽게 감지할 수 있는 고유한 로컬 기능을 형성한다.
  - 2. 이미지 및 기타 시그널의 로컬 통계는 위치에 따라 변하지 않는다.
    - 즉, 피쳐가 이미지의 한 부분에 나타날 수 있는 경우 어디에나 나타날 수 있으므로 서로 다른 위치의 유닛이 동일한 가중치를 공유하고 배열의 다른 부분에서 동일한 패턴을 감지한다는 개념이다.

- 수학적으로 피처 맵에 의해 수행되는 필터링 작업은 discrete convolution이기 때문에 이름이 이렇게 지어졌다!
- 최근 ConvNet에서는 이미지 캡션 생성을 위해 ConvNet과 RNN을 결합했다.
  <img src = ./imgs/4.png width="800" height="600">


# 6. Image understanding with deep convolutional networks (깊은 컨볼루션 네트워크를 사용한 이미지의 이해)
- 2000년대 초부터 ConvNet들은 객체 탐지, 분할에 큰 성공을 거두었다.
  - 이미지 안에서의 얼굴, 텍스트, 보행자, 인체의 탐지나, 생물학적 이미지 분할과 같이 레이블이 지정된 데이터가 상대적으로 많은 작업들이 대부분이었다.
  - ConvNet의 제일 큰 성공은 얼굴 인식이었다.

- 2012년 ImageNet 대회가 열릴 때까지 컴퓨터 비전 분야에서 거의 버려지다시피 했다.
  - 이 대회에서 1000개의 클래스가 있는 약 백만 개에 달하는 이미지에 대해서 ConvNet은 객체 인식의 오류율을 절반으로 줄였다.
  - ConvNet의 성공은 GPU, ReLU, drop-out, image augmentation 등의 효율적인 사용에서 비롯되었다.


# 7. Distributed representations and language processing (분산 표현 및 언어 처리)
- 딥러닝은 기존의 알고리즘에 비해 구성면에서 두 가지 exponential adventage가 있다.
  - 분산 표현을 학습하면 학습된 기능 값의 새로운 조합으로 generalization할 수 있다.
  - 깊은 네트워크에서 Composing layer를 구성하면 또 다른 exponential adventage를 얻을 수 있다.(깊이 면에서의 exponetial)

- 언어 모델에서 네트워크의 다른 계층은 입력 단어 벡터를 예측된 다음 단어에 대한 출력 단어 벡터로 변환하는 방법을 학습한다.
  - 이것은 어휘의 어떠 단어가 다음 단어로 나타날 확률을 예측하는 데 사용할 수 있다.

- 학습 단어 벡터는 단어 시퀀스가 실제 텍스트의 큰 말뭉치에서 나오고 규칙이 신뢰할 수 없는 경우에도 매우 잘 작동하는 것으로 나타났다.
  - 예를 들어, 뉴스 기사에서 다음 단어를 예측하도록 학습될 때 화요일과 수요일에 대해 학습된 단어 벡터는 스웨덴과 노르웨이에 대한 단어 벡터와 매우 유사하다.
  - 이러한 표현은 특징이 상호 배타적이지 않고 많은 구성이 관찰된 데이터에서 볼 수 있는 변형에 해당하기 때문에 이를 Distributed Representation(분산 표현)이라고 한다.
  - 이러한 단어 벡터는 전문가가 미리 결정해놓는게 아니라, 신경망에 의해 학습되어 자동으로 찾아진 특징으로 구성된다.

  <img src = ./imgs/5.png width="800" height="400">


# 8. Recurrent neural networks (반복 신경망)
- RNN
  - RNN은 비디오의 프레임, 문장의 단어 또는 오디오 파일의 스펙트로그램과 같은 시리즈 형식의 입력과 함께 작동하도록 설계되었다.
  - 가변 길이의 입력을 처리하고, 장기 종속성을 추적하며, 입력 데이터의 순서에 대한 정보를 유지할 수 있다.
  - 하나의 히든 레이어의 출력이 이전 레이어의 입력과 이전 시간 단계에서 동일한 위치에 있는 히든 레이어에서 전송된 상태의 함수가 되도록 구성된다.
  - 즉, 히든 레이어는 출력을 다음 레이어만이 아니라 다음 단계에까지 보낸다.
  - 아키텍처와 학습 방법의 발전 덕분에 RNN은 텍스트의 다음 문자나 시퀀스의 다음 단어를 예측하는 데 매우 능숙해졌고, 더 복잡한 작업에도 사용할 수 있게 됐다.

- LSTM
  - 이론적, 경험적 증거에 따르면, 긴 시퀀스는 시퀀스 시작 부분의 데이터가 비선형 함수를 계속 통과하면서 점차 사라질 수 있다.
  - 이 문제를 해결하기 위해 나온게 LSTM(Long Short-Term Memory)이다.
  -  LSTM은 3개의 "게이트"가 얼마나 많은 데이터를 유지하거나 버릴지, 메모리를 업데이트하는 방법 등을 제어하는 특수한 유형의 RNN이다.
  -  LSTM은 앞서 말한 문제에 대해 기존 RNN에 비해 더 효과적인 것으로 입증되었다.

  <img src = ./imgs/6.png width="400" height="400">


# 9. The future of deep learning
- 비지도 학습
  - 비지도 학습은 딥러닝에 대한 관심을 되살리는 촉매가 되었지만, 지도 학습의 성공에 가려져 버렸다.
  - 비지도 학습이 장기적으로는 훨씬 더 중요해질 것으로 예상된다.
  - 대부분 인간과 동물의 학습은 지도학습이 아니다.
  - 우리는 모든 사물의 이름을 듣는 것이 아니라, 세상을 관찰해 세상의 구조를 발견한다.

- 강화학습
  - 또, 딥러닝과 강화 학습을 결합한 시스템은 아직 초기 단계이지만, 다양한 분야에서 인상적인 결과를 보여주고 있다.

- 자연어 처리(NLP, Natural Language Processing)
  - 자연어 처리 또한 딥러닝이 큰 영향을 미칠 영역이다.

- 궁극적으로 인공지능의 발전은 표현 학습과 복잡한 추론을 결합하는 시스템을 통해 이루어질 것이다.