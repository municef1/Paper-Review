# [3] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. "Reducing the dimensionality of data with neural networks." Science 313.5786 (2006): 504-507. [pdf] (Milestone, Show the promise of deep learning)

- 이번에도 2006년 제프리 힌튼의 글이다.
- 차원축소 알고리즘으로 사용하던 주성분 분석(PCA)에서 더 효과적인 딥 오토인코더 네트워크를 사용하기 위해, 가중치를 초기화하는 효과적인 방법을 설명하는 내용이다!
- 오늘날에는 오토인코더 구조가 대단히 흔하게 써먹고 있지만, 전통적인 통계방법을 쓰는 오래된 학문 분야들은 여전히 PCA를 사용하고 있는 것으로 알고 있다.
- 단순히 딥러닝에 대한 이해보다 더 깊은 기초지식을 얻을 수 있을 것 같은 논문이다.
- 논문에 대한 이해를 돕기 위해 유튜브에 공개된 제프리 힌튼의 오픈 강의를 같이 들었다. [강의 영상](https://www.youtube.com/watch?v=PSOt7u8u23w)

- [원문 링크](https://www.science.org/doi/10.1126/science.1127647)

# 0. Abstract
1. 고차원의 입력 벡터를 저차원으로 바꾸기 위해, 중앙에 작은 레이어가 있는 다층 신경망을 사용할 수 있다.
2. 이러한 "오토인코더" 네트워크에서 가중치를 튜닝하는데 기울기 하강법을 사용할 수 있지만, 좋은 솔루션에 가까울 경우에만 제대로 작동한다.
3. 기존에 사용되던 주성분 분석(PCA)보다 딥-오토인코더 네트워크가 데이터 차원을 줄이는 도구로 사용될 수 있도록 가중치를 초기화하는 효과적인 방법을 설명한다.

# 1. 서론
- 차원 축소
  - 고차원 데이터의 분류, 시각화, 저장 등을 용이하게 해준다.
  - 흔히 사용되는 간단한 방법으로는 주성분 분석(Principal COmponents Analysis, PCA)이다.
    - PCA? : 데이터셋에서 가장 큰 분산 방향을 찾고, 각 방향을 따라 좌표로 각 데이터 포인트를 나타낸다.
- 고차원 데이터를 저차원 데이터로 변환하기 위해 adaptive한 다층 인코더 네트워크를 사용하고, 데이터를 복구하기 위해 유사한 "디코더" 네트워크를 사용하는 법을 설명할 것이다.
- 두 네트워크에서 무작위 가중치로 시작하여 원본 데이터와 재구성된 데이터 간의 불일치를 최소화하여 학습(훈련)할 수 있다.
- 필요한 그래디언트들은 chain rule을 이용해 오류의 미분값을 역전파한 다음 인코더 네트워크를 통해 쉽게 얻어낼 수 있다.

    <img src = ./imgs/1.png width="400" height="400">
- 전체 시스템을 "오토인코더"라고 하며 그림1에 묘사되어 있다.
  - (그림 1에선 2000-1000-500-30 구조를 보여주고 있지만, MNIST에선 784-1000-500-250-30를 썼다. 설명에 헷갈리지 말기!) 
- 숨겨진 레이어가 여러 개인 비선형 오토인코더는 가중치를 최적화 하는 것은 어렵다.
  - 초기 가중치가 크면 오토인코더는 일반적으로 로컬 미니멈이 좋지 않다.
  - 초기 가중치가 작으면 초기 레이어가 많은 오토인코더를 학습시키는 것이 불가능하다.
  - 초기 가중치가 좋은 솔루션에 가까우면 경사 하강법(gradient descent)이 잘 작동한다.
  - 하지만 이런 값을 찾으려면 한 번에 하나의 레이어를 학습하는 새로운 유형의 알고리즘이 필요하다.
- Binary데이터에 대한 "pretraining" 과정을 소개하고, 다양한 데이터셋에 대해 잘 작동하는 것을 증명했다.

# 2. 방법론
- MNIST 데이터셋 활용
- 784픽셀의 이미지로 시작(784는 28의 제곱이니까 아마 28*28 사이즈의 이미지였을 것 같음...?)
- 3개의 히든 레이어를 통해 중앙 레이어에서 30개의 linear unit으로 인코딩(30 linear units =30 real-valued activities라고 강의 영상에서 이야기 함)
- 30개의 real valued activities를 784개의 재구성된 픽셀로 디코딩
- 또 인코딩에 사용되는 가중치를 초기화하기 위해 RBM 스택을 사용함
- 그 다음 해당 가중치에 대한 전치를 취해 디코딩 네트워크를 초기화함
- 이렇게 처음 784개 픽셀은 인코딩에 사용된 가중치 행렬의 전치를 사용해 재구성되었지만, 4개의 RBM이 훈련되고 unroll된 후 디코딩을 위한 전치를 제공한다.
- 그 다음은 784픽셀의 재구성에 대한 오류를 최소화하기 위해 역전파 알고리즘을 적용한다.
- 이 경우 cross entropy error를 사용했다.
  - 픽셀이 logistic unit으로 표현되기 때문에!
- 이 오류값이 전체 딥 넷을 통해 다시 전파되면, 픽셀을 재구성하는데 사용되는 가중치는 픽셀을 인코딩하는데 사용되는 가중치와 달라진다.

# 3. 결과
- MNIST를 사용한 결과의 예시

    <img src = ./imgs/2.png width="800" height="400">
- 그림의 첫 줄은 각 숫자별로 하나의 샘플을 뽑은 것이다.
- 두번째 줄은 중앙의 레이어에서 30개의 linear unit을 사용하는 딥 오토 인코더에 의한 재구성 결과이다.
- 세번째는 30차원 pca, 네번째는 standard pca의 결과이다.
- 8을 봤을 때 재구성한게 PCA와 비교했을 때에 비해서 훨씬 성능이 좋다.
- mse(mean squared error, 논문에서는 average squared error라고 했으나 같은 소리임...)는 각각 3.00, 8.01, 13.87이며, 오차값이 작을수록 당연히 성능이 좋다.

# 4. 결론
- 이전에도 시도되었던 autoencoder에 대한 문제를 해결했다.
- 실제로 PCA와 비교했을 때에도 효과를 보였다.